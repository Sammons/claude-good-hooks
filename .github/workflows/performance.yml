# ===================================================================
# Performance Benchmark Workflow
# ===================================================================
# 
# Comprehensive performance testing workflow that measures:
# - Command execution times
# - Memory usage profiling
# - Bundle size monitoring
# - Performance regression detection
# - Load testing for CLI operations
#
# The workflow runs benchmarks and compares results against
# baseline performance metrics to detect regressions.
#
# Triggers:
# - Push to main branch (for baseline updates)
# - Pull requests (for regression testing)
# - Weekly scheduled runs
# - Manual dispatch
# ===================================================================

name: Performance Benchmarks

on:
  # Run on pushes to main to update baselines
  push:
    branches: [main]
  
  # Run on PRs to detect regressions
  pull_request:
    branches: [main]
  
  # Weekly performance monitoring
  schedule:
    - cron: '0 4 * * 2'  # Every Tuesday at 4 AM UTC
  
  # Allow manual triggering
  workflow_dispatch:
    inputs:
      baseline_ref:
        description: 'Baseline reference to compare against'
        required: false
        default: 'main'
      test_iterations:
        description: 'Number of test iterations'
        required: false
        default: '10'

# Minimal permissions
permissions:
  contents: read

jobs:
  # ===================================================================
  # Setup Benchmarking Environment
  # ===================================================================
  # Prepares the environment and builds all packages for testing
  # ===================================================================
  setup:
    name: Setup Benchmark Environment
    runs-on: ubuntu-latest
    outputs:
      packages: ${{ steps.packages.outputs.list }}
    
    steps:
      # Step 1: Checkout code
      - name: Checkout
        uses: actions/checkout@v4
        
      # Step 2: Setup Node.js
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20.x'
          
      # Step 3: Setup pnpm
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10.16.1
          run_install: false
          
      # Step 4: Get pnpm store directory
      - name: Get pnpm store directory
        shell: bash
        run: |
          echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV
          
      # Step 5: Setup pnpm cache
      - name: Setup pnpm cache
        uses: actions/cache@v4
        with:
          path: ${{ env.STORE_PATH }}
          key: ${{ runner.os }}-pnpm-store-${{ hashFiles('**/pnpm-lock.yaml') }}
          restore-keys: |
            ${{ runner.os }}-pnpm-store-
            
      # Step 6: Install dependencies
      - name: Install dependencies
        run: pnpm install --frozen-lockfile
        
      # Step 7: Build all packages
      - name: Build packages
        run: pnpm build
        
      # Step 8: Get list of packages for matrix
      - name: Get package list
        id: packages
        run: |
          packages=$(find packages -name package.json -not -path "*/node_modules/*" | xargs -I {} dirname {} | xargs -I {} basename {} | jq -R -s -c 'split("\n")[:-1]')
          echo "list=$packages" >> $GITHUB_OUTPUT
        
      # Step 9: Cache built artifacts
      - name: Cache build artifacts
        uses: actions/cache@v4
        with:
          path: |
            packages/*/dist
            node_modules
          key: ${{ runner.os }}-build-${{ github.sha }}

  # ===================================================================
  # Command Performance Benchmarks
  # ===================================================================
  # Measures execution time and resource usage of CLI commands
  # ===================================================================
  command-benchmarks:
    name: Command Performance
    runs-on: ubuntu-latest
    needs: setup
    strategy:
      matrix:
        package: ${{fromJson(needs.setup.outputs.packages)}}
        
    steps:
      # Step 1: Checkout code
      - name: Checkout
        uses: actions/checkout@v4
        
      # Step 2: Setup Node.js
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20.x'
          
      # Step 3: Setup pnpm
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10.16.1
          run_install: false
          
      # Step 4: Restore build cache
      - name: Restore build cache
        uses: actions/cache@v4
        with:
          path: |
            packages/*/dist
            node_modules
          key: ${{ runner.os }}-build-${{ github.sha }}
          
      # Step 5: Install performance monitoring tools
      - name: Install performance tools
        run: |
          # Install hyperfine for command benchmarking
          curl -LsSf https://github.com/sharkdp/hyperfine/releases/latest/download/hyperfine-v1.18.0-x86_64-unknown-linux-gnu.tar.gz | tar xzf -
          sudo mv hyperfine-v1.18.0-x86_64-unknown-linux-gnu/hyperfine /usr/local/bin/
          
          # Install time and other monitoring tools
          sudo apt-get update
          sudo apt-get install -y time psmisc
        
      # Step 6: Create benchmark test data
      - name: Create test data
        run: |
          mkdir -p benchmark-data
          
          # Create test hook configurations
          cat > benchmark-data/test-hooks.json << 'EOF'
          {
            "hooks": {
              "PreToolUse": [
                {
                  "matcher": "*",
                  "hooks": [
                    {
                      "type": "command",
                      "command": "echo 'Pre-tool hook executed'"
                    }
                  ]
                }
              ]
            }
          }
          EOF
          
          # Create test files for CLI operations
          for i in {1..100}; do
            echo "// Test file $i" > "benchmark-data/test-file-$i.ts"
            echo "export const test$i = 'value$i';" >> "benchmark-data/test-file-$i.ts"
          done
        
      # Step 7: Run CLI command benchmarks
      - name: Benchmark CLI commands
        run: |
          echo "â±ï¸ Running CLI command benchmarks for ${{ matrix.package }}"
          
          # Create results directory
          mkdir -p benchmark-results
          
          # Check if this package has a CLI
          if [ -f "packages/${{ matrix.package }}/package.json" ]; then
            PKG_JSON="packages/${{ matrix.package }}/package.json"
            
            # Check if package has bin entries
            if jq -e '.bin' "$PKG_JSON" > /dev/null; then
              echo "ðŸ“¦ Found CLI in ${{ matrix.package }}"
              
              # Get CLI command name
              CLI_CMD=$(jq -r '.bin | to_entries | .[0].key' "$PKG_JSON")
              CLI_PATH="packages/${{ matrix.package }}/$(jq -r '.bin | to_entries | .[0].value' "$PKG_JSON")"
              
              if [ -f "$CLI_PATH" ]; then
                echo "ðŸ” Benchmarking CLI: $CLI_CMD"
                
                # Benchmark help command
                echo "## Help Command Benchmark" > "benchmark-results/${{ matrix.package }}-commands.md"
                hyperfine --warmup 3 --runs ${{ github.event.inputs.test_iterations || '10' }} \
                  --export-json "benchmark-results/${{ matrix.package }}-help.json" \
                  --export-markdown "benchmark-results/${{ matrix.package }}-help.md" \
                  "node $CLI_PATH --help" || echo "Help command benchmark failed"
                
                cat "benchmark-results/${{ matrix.package }}-help.md" >> "benchmark-results/${{ matrix.package }}-commands.md"
                
                # Benchmark version command
                echo "## Version Command Benchmark" >> "benchmark-results/${{ matrix.package }}-commands.md"
                hyperfine --warmup 3 --runs ${{ github.event.inputs.test_iterations || '10' }} \
                  --export-json "benchmark-results/${{ matrix.package }}-version.json" \
                  --export-markdown "benchmark-results/${{ matrix.package }}-version.md" \
                  "node $CLI_PATH --version" || echo "Version command benchmark failed"
                
                cat "benchmark-results/${{ matrix.package }}-version.md" >> "benchmark-results/${{ matrix.package }}-commands.md"
                
                # Benchmark with test data (if applicable)
                if [[ "${{ matrix.package }}" == *"cli"* ]]; then
                  echo "## CLI Operations Benchmark" >> "benchmark-results/${{ matrix.package }}-commands.md"
                  
                  # Create a temporary hook config for testing
                  echo '{"hooks":{}}' > temp-config.json
                  
                  hyperfine --warmup 2 --runs 5 \
                    --export-json "benchmark-results/${{ matrix.package }}-operations.json" \
                    --export-markdown "benchmark-results/${{ matrix.package }}-operations.md" \
                    "node $CLI_PATH validate temp-config.json" || echo "CLI operations benchmark failed"
                  
                  cat "benchmark-results/${{ matrix.package }}-operations.md" >> "benchmark-results/${{ matrix.package }}-commands.md"
                  
                  rm -f temp-config.json
                fi
              else
                echo "âŒ CLI path not found: $CLI_PATH"
              fi
            else
              echo "â­ï¸ No CLI found in ${{ matrix.package }}"
            fi
          else
            echo "âŒ Package.json not found for ${{ matrix.package }}"
          fi
        
      # Step 8: Memory usage profiling
      - name: Memory usage profiling  
        run: |
          echo "ðŸ§  Profiling memory usage for ${{ matrix.package }}"
          
          if [ -f "packages/${{ matrix.package }}/package.json" ]; then
            PKG_JSON="packages/${{ matrix.package }}/package.json"
            
            if jq -e '.bin' "$PKG_JSON" > /dev/null; then
              CLI_PATH="packages/${{ matrix.package }}/$(jq -r '.bin | to_entries | .[0].value' "$PKG_JSON")"
              
              if [ -f "$CLI_PATH" ]; then
                # Profile memory usage with different operations
                echo "## Memory Usage Profile" > "benchmark-results/${{ matrix.package }}-memory.md"
                
                # Memory usage for help command
                /usr/bin/time -v node "$CLI_PATH" --help 2>&1 | grep -E "(Maximum resident set size|User time|System time)" > "benchmark-results/${{ matrix.package }}-memory-help.txt" || true
                
                # Memory usage for version command
                /usr/bin/time -v node "$CLI_PATH" --version 2>&1 | grep -E "(Maximum resident set size|User time|System time)" > "benchmark-results/${{ matrix.package }}-memory-version.txt" || true
                
                # Format memory results
                echo "### Help Command Memory Usage" >> "benchmark-results/${{ matrix.package }}-memory.md"
                if [ -f "benchmark-results/${{ matrix.package }}-memory-help.txt" ]; then
                  echo '```' >> "benchmark-results/${{ matrix.package }}-memory.md"
                  cat "benchmark-results/${{ matrix.package }}-memory-help.txt" >> "benchmark-results/${{ matrix.package }}-memory.md"
                  echo '```' >> "benchmark-results/${{ matrix.package }}-memory.md"
                fi
                
                echo "### Version Command Memory Usage" >> "benchmark-results/${{ matrix.package }}-memory.md"
                if [ -f "benchmark-results/${{ matrix.package }}-memory-version.txt" ]; then
                  echo '```' >> "benchmark-results/${{ matrix.package }}-memory.md"
                  cat "benchmark-results/${{ matrix.package }}-memory-version.txt" >> "benchmark-results/${{ matrix.package }}-memory.md"
                  echo '```' >> "benchmark-results/${{ matrix.package }}-memory.md"
                fi
              fi
            fi
          fi
        
      # Step 9: Upload benchmark results
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.package }}
          path: benchmark-results/
          retention-days: 30

  # ===================================================================
  # Bundle Size Analysis
  # ===================================================================
  # Monitors package bundle sizes and detects size regressions
  # ===================================================================
  bundle-analysis:
    name: Bundle Size Analysis
    runs-on: ubuntu-latest
    needs: setup
    
    steps:
      # Step 1: Checkout code
      - name: Checkout
        uses: actions/checkout@v4
        
      # Step 2: Setup Node.js
      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20.x'
          
      # Step 3: Setup pnpm
      - name: Setup pnpm
        uses: pnpm/action-setup@v4
        with:
          version: 10.16.1
          run_install: false
          
      # Step 4: Restore build cache
      - name: Restore build cache
        uses: actions/cache@v4
        with:
          path: |
            packages/*/dist
            node_modules
          key: ${{ runner.os }}-build-${{ github.sha }}
          
      # Step 5: Install bundle analysis tools
      - name: Install bundle analysis tools
        run: |
          npm install -g bundlesize2 package-size
        
      # Step 6: Analyze bundle sizes
      - name: Analyze bundle sizes
        run: |
          echo "ðŸ“¦ Analyzing bundle sizes..."
          
          mkdir -p bundle-analysis
          
          # Create bundle size report
          echo "# Bundle Size Analysis Report" > bundle-analysis/bundle-report.md
          echo "" >> bundle-analysis/bundle-report.md
          echo "**Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> bundle-analysis/bundle-report.md
          echo "**Commit:** ${{ github.sha }}" >> bundle-analysis/bundle-report.md
          echo "" >> bundle-analysis/bundle-report.md
          
          # Analyze each package
          for pkg in packages/*/; do
            if [ -f "$pkg/package.json" ]; then
              pkg_name=$(basename "$pkg")
              echo "## $pkg_name Package" >> bundle-analysis/bundle-report.md
              
              # Check if built files exist
              if [ -d "$pkg/dist" ]; then
                echo "### Built Files" >> bundle-analysis/bundle-report.md
                
                # List all built files with sizes
                find "$pkg/dist" -type f -name "*.js" -o -name "*.mjs" -o -name "*.cjs" | while read -r file; do
                  size=$(stat -c%s "$file" 2>/dev/null || stat -f%z "$file" 2>/dev/null || echo "0")
                  size_kb=$(( size / 1024 ))
                  echo "- \`$(basename "$file")\`: ${size_kb}KB (${size} bytes)" >> bundle-analysis/bundle-report.md
                done
                
                # Check main entry point
                main_file=$(jq -r '.main // .bin // empty' "$pkg/package.json")
                if [ -n "$main_file" ] && [ "$main_file" != "null" ]; then
                  if [ -f "$pkg/$main_file" ]; then
                    echo "### Main Entry Point Analysis" >> bundle-analysis/bundle-report.md
                    package-size "$pkg/$main_file" >> bundle-analysis/bundle-report.md 2>/dev/null || echo "Could not analyze main file" >> bundle-analysis/bundle-report.md
                  fi
                fi
              else
                echo "No built files found" >> bundle-analysis/bundle-report.md
              fi
              
              echo "" >> bundle-analysis/bundle-report.md
            fi
          done
          
          echo "âœ… Bundle size analysis completed"
        
      # Step 7: Compare with baseline (if available)
      - name: Compare with baseline
        if: github.event_name == 'pull_request'
        run: |
          echo "ðŸ“Š Comparing bundle sizes with baseline..."
          
          # This would typically fetch baseline data from a previous run
          # For now, we'll create a placeholder
          echo "## Bundle Size Comparison" >> bundle-analysis/bundle-report.md
          echo "*Bundle size comparison with baseline would appear here*" >> bundle-analysis/bundle-report.md
          echo "" >> bundle-analysis/bundle-report.md
        
      # Step 8: Upload bundle analysis
      - name: Upload bundle analysis
        uses: actions/upload-artifact@v4
        with:
          name: bundle-analysis
          path: bundle-analysis/
          retention-days: 30

  # ===================================================================
  # Performance Summary & Regression Detection
  # ===================================================================
  # Aggregates all performance data and detects regressions
  # ===================================================================
  performance-summary:
    name: Performance Summary
    runs-on: ubuntu-latest
    needs: [command-benchmarks, bundle-analysis]
    if: always()
    permissions:
      contents: read
      pull-requests: write
    
    steps:
      # Step 1: Download all benchmark results
      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: all-benchmarks/
          merge-multiple: true
        continue-on-error: true
        
      # Step 2: Download bundle analysis
      - name: Download bundle analysis
        uses: actions/download-artifact@v4
        with:
          name: bundle-analysis
          path: bundle-analysis/
        continue-on-error: true
        
      # Step 3: Generate comprehensive summary
      - name: Generate performance summary
        run: |
          echo "ðŸ“‹ Generating comprehensive performance summary..."
          
          mkdir -p performance-summary
          
          # Create summary report
          cat > performance-summary/performance-report.md << 'EOF'
          # Performance Benchmark Summary
          
          This report summarizes all performance benchmarks for the claude-good-hooks project.
          
          EOF
          
          echo "**Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> performance-summary/performance-report.md
          echo "**Commit:** ${{ github.sha }}" >> performance-summary/performance-report.md
          echo "**Branch:** ${{ github.ref_name }}" >> performance-summary/performance-report.md
          echo "" >> performance-summary/performance-report.md
          
          # Aggregate command benchmarks
          if [ -d "all-benchmarks" ]; then
            echo "## Command Performance Benchmarks" >> performance-summary/performance-report.md
            find all-benchmarks -name "*-commands.md" | while read -r file; do
              echo "### $(basename "$file" -commands.md)" >> performance-summary/performance-report.md
              cat "$file" >> performance-summary/performance-report.md
              echo "" >> performance-summary/performance-report.md
            done
            
            echo "## Memory Usage Profiles" >> performance-summary/performance-report.md
            find all-benchmarks -name "*-memory.md" | while read -r file; do
              echo "### $(basename "$file" -memory.md)" >> performance-summary/performance-report.md
              cat "$file" >> performance-summary/performance-report.md
              echo "" >> performance-summary/performance-report.md
            done
          fi
          
          # Add bundle analysis
          if [ -f "bundle-analysis/bundle-report.md" ]; then
            echo "## Bundle Size Analysis" >> performance-summary/performance-report.md
            tail -n +2 bundle-analysis/bundle-report.md >> performance-summary/performance-report.md
          fi
          
          echo "âœ… Performance summary generated"
        
      # Step 4: Comment on PR (if applicable)
      - name: Comment performance summary on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            
            try {
              const report = fs.readFileSync('performance-summary/performance-report.md', 'utf8');
              
              await github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: `## ðŸš€ Performance Benchmark Results\n\n${report}\n\n---\n*This comment was automatically generated by the performance benchmark workflow.*`
              });
            } catch (error) {
              console.log('Could not post performance summary to PR:', error);
            }
        
      # Step 5: Upload complete performance summary
      - name: Upload performance summary
        uses: actions/upload-artifact@v4
        with:
          name: performance-summary
          path: performance-summary/
          retention-days: 90